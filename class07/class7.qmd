---
title: "Class 7: Machine Learning 1"
author: "Barry (PID: 911)"
format: pdf
---

Today we will delve into unsupervised machine learning with a initial focus on clustering and dimensionality reduction.

Let's start by making up some data to cluster:
The `rnorm()` function can help us here...

```{r}
hist( rnorm(3000, mean=3) )
```

Lets get some data centered at 3,-3  -3,3

```{r}
# Combine 30 +3 values with 30 -3 values
x <- c( rnorm(30, mean=3), rnorm(30, mean=-3) )

# Bind these values togeter
z <- cbind(x=x, y=rev(x))
head(z)
```

```{r}
plot(z)
```

## K-means
Now we can see how K-means clusters this data. The main function for K-means clustering in "base R" is called `kmeans()`

```{r}
km <- kmeans(z, centers = 2)
km
```

```{r}
attributes(km)
```

> Q. What size is each cluster

```{r}
km$size
```

> Q. The cluster membership vector (i.e. the answer: cluster to which each point is allocated)

```{r}
km$cluster
```

> Q. Cluster centers

```{r}
km$centers
```

> Q. Make a results figure, i.e. plot the data `z` colored by cluster membership and show the cluster centers.

```{r}
plot(z, col="blue")
```
```{r}
plot(z, col=c("red","blue") )
```
You can specify color based on a number, where 1 is black, 2 is red

```{r}
plot(z, col=6)
```
So I can use the cluster membershp vector `km$cluster` to color up my points:

```{r}
plot(z, col=km$cluster)
points(km$centers, col="blue", pch=15)
```

> Q. Re-run your K-means clustering and as for 4 clusters and plot the results as above.

```{r}
km4 <- kmeans(z, centers = 4)
plot(z, col=km4$cluster)
points(km4$centers, col="blue", pch=15)
```

## Hierarchical Clustering

The main "base R" function for this is `hclust()`. Unlike `kmeans()` you can't just give your dataset as input, you need to provide a distance matrix. 

We can use the `dist()` function for this

```{r}
d <- dist(z)
#hclust()
```

```{r}
dim(z)
```

```{r}
hc <- hclust( d )
hc
```
There is a custom plot() for hclust objects, let's see it.

```{r}
plot(hc)
abline(h=8, col="red")
abline(h=9, col="red")
abline(h=11, col="red")

```

The function to extract clusters/grps from a hclust object/tree is called `cutree()`:

```{r}
grps <- cutree(hc, h=8)
grps
```

> Q. Plot data with hclust clusters:

```{r}
plot(z, col=grps)
```

```{r}
cutree(hc, h=8)
cutree(hc, h=9)
cutree(hc, h=10)
```

```{r}
cutree(hc, k=2)
```

# Principal Component Analysis (PCA)

The main function for PCA in base R for PCA is called `prcomp()`. There are many, many add on packages with PCA functions tailored to particular data types (RNASeq, protein structures, metagenomics, etc...)


## PCA of UK food data

Read the data into R, it is a CSV file and we can use `read.csv()` to read it:

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

I would the food names as row names not their own colum of data (first column curently). I can fix this like so:

```{r}
rownames(x) <- x[,1]
y <- x[,-1]
y
```

A better way to do this is to do it at the time of data import with `read.csv()`

```{r}
food <- read.csv(url, row.names = 1)
food
```

Let's make some plots and dig into the data a little.

```{r}
rainbow(nrow(food))
```

```{r}
barplot(as.matrix(food), beside=F, col=rainbow(nrow(food)))
```

```{r}
barplot(as.matrix(t(food)), beside=T)
```

How a bout a so-called "pairs" plot where we plot each country against all other countries.

```{r}
pairs(food, col=rainbow(nrow(food)), pch=16)
```

Really there has to be a better way....

## PCA to the rescue!

We can run a Principal Component Analysis (PCA) fo this data with the `prcomp()` function.

```{r}
head(food)
```

We need to take the transpose of this data to get the foods in the columns and the countries in the rows

```{r}
pca <- prcomp( t(food) )
summary(pca)
```

What is in my `pca` result object?

```{r}
attributes(pca)
```
The scores along the new PCs
```{r}
pca$x
```
To make my main result figure, often called a PC plot (or score plot, ordenation plot, or PC1 vs PC2 plot etc.)

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2",
     col=c("orange","red","blue", "darkgreen"), pch=16)
```


```{r}
library(ggplot2)

data <- as.data.frame(pca$x)

ggplot(data) +
  aes(PC1, PC2) +
  geom_point(col=c("orange","red","blue", "darkgreen"))

```

To see the contributions of the original variables (foods) to these new PCs we can look at the `pca$rotation` component of our results object.

```{r}
loadings <- as.data.frame(pca$rotation)
loadings$name <- rownames(loadings)

ggplot(loadings) +
  aes(PC1, name) +
  geom_col()
```

And PC2

```{r}
ggplot(loadings) +
  aes(PC2, name) +
  geom_col()
```

## Scalling

```{r}
head(mtcars)
```
```{r}
pc1 <- prcomp(mtcars)
pc2 <- prcomp(mtcars, scale=T)
```

```{r}
biplot(pc1)
```

```{r}
biplot(pc2)
```

```{r}
p1load <- as.data.frame(pc1$rotation)
p2load <- as.data.frame(pc2$rotation)

p1load$name <- rownames(pc1$rotation)
p2load$name <- rownames(pc2$rotation)

```

```{r}
p1 <- ggplot(p1load) +
  aes(PC1, name) +
  geom_col() +
  xlim(c(-1,1)) +
  ylab("") +
  xlab("PC1 loadings (Scale=FALSE)")
```

```{r}
p2 <- ggplot(p2load) +
  aes(PC1, name) +
  geom_col() +
  xlim(c(-1,1)) +
  ylab("") +
  xlab("PC1 loadings (Scale=TRUE)")
```

```{r}
ggsave("nonscale.png", p1)
ggsave("scale.png", p2)
```


```{r}
library(patchwork)

(p1 | p2)
```

